{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 核心概念\n",
    "- 一个HDF5文件是一种存放两类对象的容器：dataset和group. Dataset是类似于数组的数据集，而group是类似文件夹一样的容器，存放dataset和其他group。在使用h5py的时候需要牢记一句话：groups类比词典，dataset类比Numpy中的数组。 \n",
    "- HDF5的dataset虽然与Numpy的数组在接口上很相近，但是支持更多对外透明的存储特征，如数据压缩，误差检测，分块传输。\n",
    "\n",
    "- 在使用Matlab对数据进行预处理时，遇到了内存不足的问题，因为数据量太大，在处理完成以前内存已经爆满。如果使用Matlab的.m文件对文件进行存储的话，则需要将数据分割成多个文件，对后续的处理造成了不便。HDF5文件则是一种灵活的文件存储格式，有一个最大的好处就是在Matlab的处理过程中可以对它进行扩展写入，也就是说不是所有数据处理完以后一次写入，而是边处理边写入，极大的降低了对系统内存的要求。\n",
    "\n",
    "- HDF5文件类似与一个文件系统，使用这个文件本身就可以对数据集（dataset）进行管理。例如下图所示，HDF5文件中的数据集皆存储根目录/，在根目录下存在多个group，这样一些group类似与文件系统的文件夹，在它们可以存储别的group，也可以存储数据集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![OO1](https://github.com/Alxemade/Screenshots/blob/master/20181126/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建和保存文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 创建一个HDF5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py  # 首先我们导入h5dy的包\n",
    "import numpy as np\n",
    "f = h5py.File(\"mytestfile.hdf5\", \"w\")  # 首先我们以'w'模式写入一个文件，然后保存为mytestfile.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 创建一个dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以借助文件对象的一系列方法添加数据。其中create_dataset用户创建给定形状和数据类型的空dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"mydataset\", (100,), dtype='i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create_dataset(name, shape=None, dtype=None, data=None, **kwds)\n",
    "\n",
    "Create a new dataset. Options are explained in Creating datasets.\n",
    "\n",
    "**Parameters:**\t\n",
    "- name – Name of dataset to create. May be an absolute or relative path. Provide None to create an anonymous dataset, to be linked into the file later.\n",
    "- shape – Shape of new dataset (Tuple).\n",
    "- dtype – Data type for new dataset\n",
    "- data – Initialize dataset to this (NumPy array).\n",
    "- chunks – Chunk shape, or True to enable auto-chunking.\n",
    "- maxshape – Dataset will be resizable up to this shape (Tuple). Automatically enables chunking. Use None for the axes you want to be unlimited.\n",
    "- compression – Compression strategy. See Filter pipeline.\n",
    "- compression_opts – Parameters for compression filter.\n",
    "- scaleoffset – See Scale-Offset filter.\n",
    "- shuffle – Enable shuffle filter (T/F). See Shuffle filter.\n",
    "- fletcher32 – Enable Fletcher32 checksum (T/F). See Fletcher32 filter.\n",
    "- fillvalue – This value will be used when reading uninitialized parts of the dataset.\n",
    "- track_times – Enable dataset creation timestamps (T/F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用现有Numpy数组来初始化一个dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.arange(100)\n",
    "dset = f.create_dataset(\"init\", data=arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 分块存储策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在缺省设置下，HDF5数据集在内存中是连续布局的，也就是按照传统的C序。Dataset也可以在HDF5的分块存储布局下创建。也就是dataset被分为大小相同的若干块随意地分布在磁盘上，并使用B树建立索引。 \n",
    "\n",
    "为了进行分块存储，将关键字设为一个元组来指示块的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"chunked\",(1000,1000),chunks=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以自动分块，不必指定快的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset  = f.create_dataset(\"autochunk\",(1000,1000), chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 读取HDF5文件的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('mytestfile.hdf5','r')  # 这里是'r'表示我们读取hdf5里面的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请记住htpy.File类似python的dict对象，因此我们可以查看所有的键值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<HDF5 file \"mytestfile.hdf5\" (mode r+)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于以上观测，文件中有名字为mydataset这样一个数据集。然后我们可以用类似词典的方法读取对应的dataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = f['mydataset']  # 先读取\n",
    "dset.shape  # 我们可以输出他的维度\n",
    "dest.dtype # 也可以得到的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 HDF5的分层结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“HDF”代表\"Hiearchical Data Format(分层数据格式)\".hdf5文件中group类似于文件夹，我们创建的文件对象本身就是一个group，称为root group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.name  # 因为这里我们还没有创建name，所以暂时为空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建subgroup是使用create_group的方法实现的，但是我们首先需要用读写模式打开文件：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('mytestfile.hdf5','r+')\n",
    "grp = f.create_group('subgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在group中跌倒就可以得到group内所有的直接附属的成员，包含dataset和subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autochunk\n",
      "init\n",
      "mydataset\n",
      "subgroup\n"
     ]
    }
   ],
   "source": [
    "for name in f:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5的一个很棒的特点就是你可以在数据旁边存储元数据,所有的group和dataset都支持叫做属性的数据格式,属性通过attrs成员访问，类似与python词典格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.attrs['temperature'] = 99.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.attrs['temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 高级特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 滤波器组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HDF5的滤波器组可以对分块数组进行变换，最常用的变换是高保真压缩,使用一个特定的压缩滤波器创建dataset之后，读写都可以向平常一样，不必添加额外的步骤。\n",
    "* 用管检测compression来指定压缩滤波器,而滤波器可以利用关键词compression_opt指定：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset('zipped', (100,100),compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HDF5文件的限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HDF5文件本身大小没有限制，但是HDF5的一个dataset最高允许32个维，每个维度最多可有2^64个值，每个值大小理论上可以任意大  \n",
    "- 目前一个chunk允许的最大容量为2^32-1 byte (4GB). 大小固定的dataset的块的大小不能超过dataset的大小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [h5py quick start guide](http://docs.h5py.org/en/latest/quick.html)\n",
    "2. [python开源库——h5py快速指南](https://blog.csdn.net/yudf2010/article/details/50353292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
