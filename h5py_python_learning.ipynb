{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 核心概念\n",
    "\n",
    "- 在使用Matlab对数据进行预处理时，遇到了内存不足的问题，因为数据量太大，在处理完成以前内存已经爆满。如果使用Matlab的.m文件对文件进行存储的话，则需要将数据分割成多个文件，对后续的处理造成了不便。HDF5文件则是一种灵活的文件存储格式，有一个最大的好处就是在Matlab的处理过程中可以对它进行扩展写入，也就是说不是所有数据处理完以后一次写入，而是边处理边写入，极大的降低了对系统内存的要求。\n",
    "\n",
    "- HDF5文件类似与一个文件系统，使用这个文件本身就可以对数据集（dataset）进行管理。例如下图所示，HDF5文件中的数据集皆存储根目录/，在根目录下存在多个group，这样一些group类似与文件系统的文件夹，在它们可以存储别的group，也可以存储数据集。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture/001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对于每一个dataset 而言，除了数据本身之外，这个数据集还会有很多的属性 attribute.在hdf5中，还同时支持存储数据集对应的属性信息，所有的属性信息的集合就叫做metadata;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture/002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 使用matlab处理相关hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 创建文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Matlab创建HDF5文件的函数是h5create，使用如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h5create(filename,datasetname,[30, 30 , 3, inf],'Datatype','single','ChunkSize',[30,30,3,1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filename为h5文件的文件名（不知道什么问题，在我的电脑上使用时，这个函数无法指定路径）。\n",
    "- datasetname则为数据集的名字，数据集名称必须以/开头，比如/G。\n",
    " [30,30,3,inf]位数据集的大小，比如我的数据集为30x30大小的彩色图像，并且我希望数量能够扩展，那么就可以指定最后以为度为inf，以表示数量不限。\n",
    "- Datatype为数据类型\n",
    "- ChunkSize为数据存储的最小分块，为了让数据能够具有扩展性，所以为新来的数据分配一定的空间大小，对于一个非常大的数据，这个值设置大一点比较好，这样分块就会少一点。比如我的数据集中，30x30大小的彩色图像大概有10万个左右，那么1000个存储在一起较为合适，则chunksize设置为：[30,30,3,1000]。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 使用Matlab写入HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在创建了hdf5文件和数据集以后，则可以对数据集进行写操作以扩展里面的数据。使用Matlab写入HDF5文件的函数是h5write，使用如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> h5write(fileName,datasetName,data,start,count);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fileName: hdf5文件名\n",
    "- datasetName:数据集名称，比如/G\n",
    "- data:需要写入的数据，数据的维度应该与创建时一致，比如，设置的数据集大小为[30,30,3,inf]，那么这里的data的前三个维度就应该是[30, 30, 3]，而最后一个维度则是自由的\n",
    "- start:数据存储的起点，如果是第一次存，则应该为[1, 1, 1, 1]（注意数据维度的一致性），如果这次存了10000个样本，也就是[30,30,3,10000]，那么第二次存储的时候起点就应该为[1,1,1,10001]\n",
    "- count存储数据的个数，同样要根据维度来（其实就是数据的维度），这里为[30,30,3,10000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 使用Matlab读取HDF5中的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matlab中可以使用h5info函数来读取HDF5文件的信息："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fileInfo = h5info(fileName);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后通过解析fileInfo结构，则可以得到HDF5文件中的数据集名称、数据集大小等等必要信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 使用Matlab读取HDF5中的数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matlab中可以使用h5read函数来读取HDF5文件："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> data = h5read(filename,datasetname,start,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filename：HDF5文件文件名\n",
    "- datasetname：数据集名称\n",
    "- start：从数据集中取数据的其实位置\n",
    "- count：取的数据数量\n",
    "- 还是以上面的30x30的彩色图像为例，如果每次需要取1000个，那么第一次取时，start应该设置为[1, 1, 1, 1] ，count设置为：[30, 30 ,3 1000]。第二次取值时，start则应该设置为[1, 1, 1, 1001]，count则设置为：[30, 30, 3, 1000]。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 使用python处理hdf5文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一个HDF5文件是一种存放两类对象的容器：dataset和group. Dataset是类似于数组的数据集，而group是类似文件夹一样的容器，存放dataset和其他group。在使用h5py的时候需要牢记一句话：groups类比词典，dataset类比Numpy中的数组。 \n",
    "- HDF5的dataset虽然与Numpy的数组在接口上很相近，但是支持更多对外透明的存储特征，如数据压缩，误差检测，分块传输。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 创建和保存文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 创建一个HDF5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py  # 首先我们导入h5dy的包\n",
    "import numpy as np\n",
    "f = h5py.File(\"mytestfile.hdf5\", \"w\")  # 首先我们以'w'模式写入一个文件，然后保存为mytestfile.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.1 创建一个dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以借助文件对象的一系列方法添加数据。其中create_dataset用户创建给定形状和数据类型的空dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"mydataset\", (100,), dtype='i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create_dataset(name, shape=None, dtype=None, data=None, **kwds)\n",
    "\n",
    "Create a new dataset. Options are explained in Creating datasets.\n",
    "\n",
    "**Parameters:**\t\n",
    "- name – Name of dataset to create. May be an absolute or relative path. Provide None to create an anonymous dataset, to be linked into the file later.\n",
    "- shape – Shape of new dataset (Tuple).\n",
    "- dtype – Data type for new dataset\n",
    "- data – Initialize dataset to this (NumPy array).\n",
    "- chunks – Chunk shape, or True to enable auto-chunking.\n",
    "- maxshape – Dataset will be resizable up to this shape (Tuple). Automatically enables chunking. Use None for the axes you want to be unlimited.\n",
    "- compression – Compression strategy. See Filter pipeline.\n",
    "- compression_opts – Parameters for compression filter.\n",
    "- scaleoffset – See Scale-Offset filter.\n",
    "- shuffle – Enable shuffle filter (T/F). See Shuffle filter.\n",
    "- fletcher32 – Enable Fletcher32 checksum (T/F). See Fletcher32 filter.\n",
    "- fillvalue – This value will be used when reading uninitialized parts of the dataset.\n",
    "- track_times – Enable dataset creation timestamps (T/F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以使用现有Numpy数组来初始化一个dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr = np.arange(100)\n",
    "dset = f.create_dataset(\"init\", data=arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.2 分块存储策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在缺省设置下，HDF5数据集在内存中是连续布局的，也就是按照传统的C序。Dataset也可以在HDF5的分块存储布局下创建。也就是dataset被分为大小相同的若干块随意地分布在磁盘上，并使用B树建立索引。 \n",
    "\n",
    "为了进行分块存储，将关键字设为一个元组来指示块的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset(\"chunked\",(1000,1000),chunks=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以自动分块，不必指定快的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset  = f.create_dataset(\"autochunk\",(1000,1000), chunks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 读取HDF5文件的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "f = h5py.File('mytestfile.hdf5','r')  # 这里是'r'表示我们读取hdf5里面的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请记住htpy.File类似python的dict对象，因此我们可以查看所有的键值：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeysView(<HDF5 file \"mytestfile.hdf5\" (mode r+)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于以上观测，文件中有名字为mydataset这样一个数据集。然后我们可以用类似词典的方法读取对应的dataset对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset = f['mydataset']  # 先读取\n",
    "dset.shape  # 我们可以输出他的维度\n",
    "dest.dtype # 也可以得到的数据类型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 HDF5的分层结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“HDF”代表\"Hiearchical Data Format(分层数据格式)\".hdf5文件中group类似于文件夹，我们创建的文件对象本身就是一个group，称为root group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.name  # 因为这里我们还没有创建name，所以暂时为空"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建subgroup是使用create_group的方法实现的，但是我们首先需要用读写模式打开文件：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File('mytestfile.hdf5','r+')\n",
    "grp = f.create_group('subgroup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在group中跌倒就可以得到group内所有的直接附属的成员，包含dataset和subgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autochunk\n",
      "init\n",
      "mydataset\n",
      "subgroup\n"
     ]
    }
   ],
   "source": [
    "for name in f:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5的一个很棒的特点就是你可以在数据旁边存储元数据,所有的group和dataset都支持叫做属性的数据格式,属性通过attrs成员访问，类似与python词典格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset.attrs['temperature'] = 99.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.attrs['temperature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 高级特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 滤波器组"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* HDF5的滤波器组可以对分块数组进行变换，最常用的变换是高保真压缩,使用一个特定的压缩滤波器创建dataset之后，读写都可以向平常一样，不必添加额外的步骤。\n",
    "* 用管检测compression来指定压缩滤波器,而滤波器可以利用关键词compression_opt指定：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset = f.create_dataset('zipped', (100,100),compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 HDF5文件的限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HDF5文件本身大小没有限制，但是HDF5的一个dataset最高允许32个维，每个维度最多可有2^64个值，每个值大小理论上可以任意大  \n",
    "- 目前一个chunk允许的最大容量为2^32-1 byte (4GB). 大小固定的dataset的块的大小不能超过dataset的大小。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 参考文献"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [h5py quick start guide](http://docs.h5py.org/en/latest/quick.html)\n",
    "2. [python开源库——h5py快速指南](https://blog.csdn.net/yudf2010/article/details/50353292)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
